\documentclass[11pt]{article}

% Packages
\usepackage{amscd}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{bbold}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{color}
\usepackage{easybmat}
\usepackage{etex}
\usepackage{framed}
\usepackage[dvips,letterpaper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage{mathtools}
\usepackage{setspace}
\usepackage{verbatim}
\usepackage[capitalize]{cleveref}
\usepackage{autonum}

%
% Commands
%

% Figures
\newcommand{\fig}[1]{(figure \ref{#1})}
\newcommand{\FIG}[1]{figure \ref{#1}}

\DeclareSymbolFont{bbold}{U}{bbold}{m}{n}
\DeclareSymbolFontAlphabet{\mathbbold}{bbold}

% Names
\newcommand{\Mobius}{M\"{o}bius}
\newcommand{\Holder}{H\"{o}lder}
\newcommand{\Rouche}{Rouch\'{e}}
\newcommand{\Ito}{It\={o}}
\newcommand{\Kondo}{Kond\^{o}}
\newcommand{\Levy}{L\'{e}vy}
\newcommand{\Cramer}{Cram\'{e}r}
\newcommand{\Godel}{G\"{o}del}
\newcommand{\Carath}{Carath\'{e}odory}
\newcommand{\Caratheodory}{Carath\'{e}odory}
\newcommand{\Hopital}{H\^{o}pital}

% Random
\renewcommand{\bar}{\overline}
\newcommand{\lvec}{\overrightarrow}
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}

% Disjoint union
\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand{\charfusion}[3][\mathord]{
    #1{\ifx#1\mathop\vphantom{#2}\fi
        \mathpalette\mov@rlay{#2\cr#3}
      }
    \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother

\newcommand{\cupdot}{\charfusion[\mathbin]{\cup}{\cdot}}
\newcommand{\bigcupdot}{\charfusion[\mathop]{\bigcup}{\cdot}}

% Blackboard bold
\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\DD}{\mathbb{D}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\FF}{\mathbb{F}}
\renewcommand{\SS}{\mathbb{S}}
\newcommand{\Fp}{\FF_p}
\newcommand{\TrivGp}{\mathbbold{1}}
\newcommand{\One}{\mathbbold{1}}

\newcommand{\RP}{\RR\mathrm{P}}
\newcommand{\CP}{\CC\mathrm{P}}

% Vector bold
\newcommand{\nn}{\bm{n}}
\newcommand{\vv}{\bm{v}}
\newcommand{\ww}{\bm{w}}
\newcommand{\xx}{\bm{x}}
\newcommand{\yy}{\bm{y}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\one}{\bm{1}}

% Other fonts
\newcommand{\fp}{\mathfrak{p}}
\newcommand{\fq}{\mathfrak{q}}
\newcommand{\fg}{\mathfrak{g}}
\newcommand{\fh}{\mathfrak{h}}
\newcommand{\fa}{\mathfrak{a}}
\newcommand{\fb}{\mathfrak{b}}
\newcommand{\fc}{\mathfrak{c}}
\newcommand{\fm}{\mathfrak{m}}
\renewcommand{\sl}{\mathfrak{sl}}
\newcommand{\so}{\mathfrak{so}}
\newcommand{\gl}{\mathfrak{gl}}
\renewcommand{\sp}{\mathfrak{sp}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\sB}{\mathcal{B}}
\newcommand{\sC}{\mathcal{C}}
\newcommand{\sD}{\mathcal{D}}
\newcommand{\sE}{\mathcal{E}}
\newcommand{\sF}{\mathcal{F}}
\newcommand{\sG}{\mathcal{G}}
\newcommand{\sH}{\mathcal{H}}
\newcommand{\sI}{\mathcal{I}}
\newcommand{\sL}{\mathcal{L}}
\newcommand{\sM}{\mathcal{M}}
\newcommand{\sN}{\mathcal{N}}
\newcommand{\sO}{\mathcal{O}}
\newcommand{\sP}{\mathcal{P}}
\newcommand{\sR}{\mathcal{R}}
\newcommand{\sS}{\mathcal{S}}
\newcommand{\sT}{\mathcal{T}}
\newcommand{\sU}{\mathcal{U}}
\newcommand{\sV}{\mathcal{V}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}

% Spacing
\newcommand\ThmBr{%
    \@ifstar{\item[\vbox{\null}]}{%
      \begingroup % keep changes local
      \setlength\itemsep{0pt}%
      \setlength\parsep{0pt}%
       \item[\vbox{\null}]%
      \endgroup%
     }}
\newcommand{\br}{\vspace{1pc}}
\newcommand{\BR}{\vspace{2pc}}
\newcommand{\picspace}{\vspace{13pc}}
\newcommand{\hs}{\hspace{1mm}}
\newcommand{\HS}{\hspace{3.5mm}}
\newcommand{\hr}{
  \begin{center}
    \line(1,0){250}
  \end{center}
}
\newcommand{\hrs}{
  \begin{center}
    \line(1,0){150}
  \end{center}
}

% Plain text
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\newcommand{\disc}{\mathrm{disc}}
\newcommand{\Ann}{\mathrm{Ann}}
\newcommand{\Ass}{\mathrm{Ass}}
\newcommand{\Soc}{\mathrm{Soc}}
\newcommand{\Supp}{\mathrm{Supp}}
\newcommand{\Spec}{\mathrm{Spec}}
\newcommand{\maxSpec}{\mathrm{maxSpec}}

\newcommand{\N}{\mathrm{N}}
\newcommand{\Tr}{\mathrm{Tr}}

% Functors
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\Der}{\mathrm{Der}}
\newcommand{\End}{\mathrm{End}}
\newcommand{\Ind}{\mathrm{Ind}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Gal}{\mathrm{Gal}}
\newcommand{\Sym}{\mathrm{Sym}}
\newcommand{\Rad}{\mathrm{Rad}}
\newcommand{\Id}{\mathrm{Id}}
\newcommand{\Ad}{\mathrm{Ad}}
\newcommand{\ad}{\mathrm{ad}}
\newcommand{\Pow}{\mathrm{Pow}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\img}{\mathrm{img}}
\newcommand{\sgn}{\mathrm{sgn}}

\newcommand{\ch}{\mathrm{char}}
\newcommand{\Res}{\mathrm{Res}}
\newcommand{\ord}{\mathrm{ord}}
\newcommand{\cont}{\mathrm{cont}}
\newcommand{\ab}{\mathrm{ab}}
\newcommand{\Orb}{\mathrm{Orb}}
\newcommand{\Syl}{\mathrm{Syl}}
\newcommand{\Irr}{\mathrm{Irr}}
\newcommand{\Frac}{\mathrm{Frac}}
\newcommand{\sep}{\mathrm{sep}}
\newcommand{\per}{\mathrm{per}}

% Groups
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\PGL}{\mathrm{PGL}}
\newcommand{\PSL}{\mathrm{PSL}}
\newcommand{\SL}{\mathrm{SL}}
\newcommand{\oO}{\mathrm{O}}
\newcommand{\SO}{\mathrm{SO}}
\newcommand{\PSO}{\mathrm{PSO}}
\newcommand{\Sp}{\mathrm{Sp}}
\newcommand{\PSp}{\mathrm{PSp}}
\newcommand{\U}{\mathrm{U}}
\newcommand{\SU}{\mathrm{SU}}
\newcommand{\PSU}{\mathrm{PSU}}

% Parentheses
\newcommand{\lgndr}[2]{\ensuremath{\left(\frac{#1}{#2}\right)}}

% Mappings
\newcommand{\iso}{\cong}
\newcommand{\eqdf}{\stackrel{\mathrm{df}}{=}}
\newcommand{\eqd}{\stackrel{\mathrm{d}}{=}}
\newcommand{\eqqu}{\stackrel{\mathrm{?}}{=}}
\newcommand{\xto}{\xrightarrow}
\newcommand{\dto}{\Rightarrow}
\newcommand{\into}{\hookrightarrow}
\newcommand{\xinto}{\xhookrightarrow}
\newcommand{\onto}{\twoheadrightarrow}
\newcommand{\xonto}{xtwoheadrightarrow}
\newcommand{\isoto}{\xto{\sim}}
\newcommand{\upto}{\nearrow}
\newcommand{\downto}{\searrow}

% Convenience
\newcommand{\Implies}{\ensuremath{\Rightarrow}}
\newcommand{\ImpliedBy}{\ensuremath{\Leftarrow}}
\newcommand{\Iff}{\ensuremath{\Leftrightarrow}}

\newcommand{\Pfright}{\ensuremath{(\Rightarrow):\hs}}
\newcommand{\Pfleft}{\ensuremath{(\Leftarrow):\hs}}

\newcommand{\sm}{\ensuremath{\setminus}}

\newcommand{\tab}[1]{(table \ref{#1})}
\newcommand{\TAB}[1]{table \ref{#1}}

\newcommand{\precode}[1]{\textbf{\footnotesize #1}}
\newcommand{\code}[1]{\texttt{\footnotesize #1}}

\newcommand{\sectionline}{
  \nointerlineskip \vspace{\baselineskip}
  \hspace{\fill}\rule{0.35\linewidth}{.7pt}\hspace{\fill}
  \par\nointerlineskip \vspace{\baselineskip}
}


%
% Misc
%

\parskip0em
\linespread{1.05}
\widowpenalty10000
\clubpenalty10000


\newcommand{\HC}{\mathsf{HC}}
\newcommand{\CHC}{\mathsf{CHC}}
\newcommand{\SK}{\mathsf{SK}}
\newcommand{\SDP}{\mathsf{SDP}}
\newcommand{\SOS}{\mathsf{SOS}}
\newcommand{\PE}{\mathsf{PE}}
\newcommand{\PS}{\mathsf{PS}}
\newcommand{\tEE}{\tilde{\mathbb{E}}}
\newcommand{\tCov}{\widetilde{\mathrm{Cov}}}
\newcommand{\sfP}{\mathsf{P}}
\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\diag}{\mathsf{diag}}
\newcommand{\vrad}{\mathrm{vrad}}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\rkeff}{\mathrm{rk}_{\mathrm{eff}}}
\newcommand{\GOE}{\mathsf{GOE}}
\newcommand{\ssG}{\mathsf{G}}
\newcommand{\balpha}{\bm \alpha}
\newcommand{\blambda}{\bm \lambda}
\newcommand{\bbeta}{\bm \beta}
\newcommand{\bA}{\bm A}
\newcommand{\bB}{\bm B}
\newcommand{\bD}{\bm D}
\newcommand{\bF}{\bm F}
\newcommand{\bG}{\bm G}
\newcommand{\bH}{\bm H}
\newcommand{\bP}{\bm P}
\newcommand{\bQ}{\bm Q}
\newcommand{\bR}{\bm R}
\newcommand{\bV}{\bm V}
\newcommand{\bW}{\bm W}
\newcommand{\bX}{\bm X}
\newcommand{\bY}{\bm Y}
\newcommand{\ba}{\bm a}
\newcommand{\bb}{\bm b}
\newcommand{\bc}{\bm c}
\newcommand{\bg}{\bm g}
\newcommand{\bv}{\bm v}
\newcommand{\bw}{\bm w}
\newcommand{\bx}{\bm x}

\newcommand{\bmat}[2]{
	\begin{bmatrix*}[#1]
		#2
	\end{bmatrix*}
}

\DeclareRobustCommand{\bmrob}[1]{\bm{#1}}
\pdfstringdefDisableCommands{%
  \renewcommand{\bmrob}[1]{#1}%
}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{question}{Question}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

\title{Super-Resolution of Point Sources Down to the Rayleigh Limit from Multiple Observations}
\author{Dmitriy Kunisky, Efe Onaran}

\begin{document}

\maketitle

\noindent

\section{Introduction}

\subsection{Single Observation Problem Statement}

Suppose we observe a true signal whose representation in the physical domain is
\begin{equation}
    x^*(t) = \sum_{j = 1}^s a_j \delta_{t_j}(t),
\end{equation}
for $t \in \TT$ where $\TT = \RR / \ZZ$, which we will usually think of as the unit interval $[0, 1]$ with its endpoints identified, and $T = \{t_j\}_{j = 1}^s \subset \TT$ a discrete support set.
In the Fourier domain, this signal takes the form
\begin{equation}
    \widehat{x^*}(k) = \sum_{j = 1}^s a_j \exp(-2\pi i k t_j)
\end{equation}
for $k \in \ZZ$.

Now, we are interested in recovering this true signal $x$ from an observation that suffers from low resolution, which we represent as convolution of $x(t)$ with a point-spread function (PSF) denoted by $\phi(t)$.
We then observe the signal
\begin{equation}
    (\phi * x^*)(t) = \sum_{j = 1}^sa_j \phi(t - t_j).
\end{equation}
In the simplest case, $\phi$ is a Dirichlet kernel with cutoff frequency $f_c$, in which case the Fourier transform of the above is simply the truncation of the Fourier transform of $x^*$.
Denoting by $y$ the signal we observe in the Fourier domain, we have
\begin{equation}
    y(k) = \left(\sum_{j = 1}^s a_j \exp(-2\pi i k t_j)\right)\One\{|k| \leq f_c \}.
\end{equation}
We may then think of the data we observe as simply the $2f_c + 1$ values $y(-f_c), \dots, y(f_c)$.
We write $\sF_n$ for the sensing operator mapping $x^*$ to these $n$ Fourier coefficients.
A popular technique for solving this problem is \emph{total variation minimization}, which attempts to recover $x^*$ by solving the convex problem
\begin{equation}
    \argmin_x \|x\|_{\mathsf{TV}} \text{ subject to } \sF_nx = y.
\end{equation}

\subsection{Extension to Multiple Observations}

We now consider a generalization of the problem presented in the previous section, where we make \emph{several} observations signals sharing the support $T = \{t_1, \dots, t_s\}$ of $x^*$, but having varying amplitudes $a_j$.
To formalize this, our true signal is now
\begin{equation}
    x^*_\ell(t) = \sum_{j = 1}^s a_{\ell, j} \delta_{t_j} \text{ for } \ell \in \{1, \dots, m\},
\end{equation}
and our observations are $y_{\ell, k} = (\sF_n x^*_\ell)_{k}$ for $k \in \{-f_c, \dots, f_c\}$ and $\ell \in \{1, \dots, m\}$.
We think of the $a_{\ell, j}$ as organized into vectors $\ba_{j} \in \RR^m$ for $j \in \{1, \dots, s\}$, and the  $y_{\ell, k}$ as organized into a matrix $\bY \in \CC^{m \times n}$.
The \emph{group total variation minimization} is the natural extension of the previous convex problem to this setting, where we solve
\begin{equation}
    \argmin_x \|x\|_{\mathsf{gTV}} \text{ subject to } \left[\begin{array}{cccc}\sF_nx_1 & \sF_n x_2 \cdots \sF_n x_m \end{array}\right]^\top = \bY.
\end{equation}

\subsection{Dual Certificates}

In this section, we lay out the Lagrangian duality theory for TV and gTV norm minimization, the main tool for theoretical analysis of the performance of these algorithms.

\begin{definition}
    Let $\mu^0 \subset \CC$ denote the complex unit circle.
    A \emph{sign pattern} on a set is an assignment of points of $\mu^0$ to each point of the set.
\end{definition}

\begin{definition}
    For a sign pattern $v \in (\mu^0)^T$, a low-pass trigonometric polynomial $q: \TT \to \CC$,
    \begin{equation}
        q(t) = \sum_{k = -f_c}^{f_c} c_k e^{2\pi i k t},
    \end{equation}
    is a \emph{single-observation dual certificate} for $v$ if $q(t_j) = v_j$ for $t_j \in T$ and $|q(t)| < 1$ for $t \notin T$.
\end{definition}
\begin{proposition}[TV Norm Minimization Duality]
If a dual certificate for the sign pattern $v_j = a_j / |a_j|$ exists, then $x^*$ is the unique solution of TV minimization for the super-resolution problem.
\end{proposition}
Therefore, to prove the effectiveness of TV minimization it suffices to show that a dual certificate exists under some conditions on $T$.
The main result of \cite{fernandez2016super} establishes that this is true under a minimum separation condition.
\begin{theorem}[Proposition 2.3 of \cite{fernandez2016super}]
    If $\Delta(T) \geq 1.26\lambda_c$ and $f_c \geq 10^3$, then a dual certificate exists for any sign pattern on $T$.
    \label{thm:single-obs-recovery}
\end{theorem}

As also observed in \cite{fernandez2016super}, the same ideas extend to the multiple observation case in a straightforward fashion.
\begin{definition}
    Let $\mu^{m - 1} \subset \CC^m$ denote the $m$-dimensional complex unit sphere.
    A \emph{$m$-dimensional sign pattern} on a set is an assignment of points of $\mu^{m - 1}$ to each point of the set.
\end{definition}
\begin{definition}
    For a sign pattern $\bv \in (\mu^{m - 1})^T$, a low-pass trigonometric polynomial $q: \TT \to \CC^m$ given by $q(t) = (q_1(t), \dots, q_m(t))$ and
    \begin{equation}
        q_\ell(t) = \sum_{k = -f_c}^{f_c} c_k \exp(2\pi i k t) \text{ for } \ell \in \{1, \dots, m\}
    \end{equation}
    is an \emph{$m$-observation dual certificate} for $v$ if $q(t_j) = \bv_j$ for $t_j \in T$, and $\|q(t)\|_{\ell^2} < 1$ for $t \notin T$.
\end{definition}
\begin{proposition}[gTV Norm Minimization Duality]
    If a dual certificate for the sign pattern $\bv_j = \ba_{j} / \|\ba_{j}\|_{\ell^2}$ exists, then $x^*$ is the unique solution of gTV minimization for the super-resolution problem.
\end{proposition}
However, while numerical experiments in \cite{fernandez2016super} suggest that when the amplitude vectors $\ba_j$ are taken randomly then recovery is possible down to a lower critical minimum separation which accumulates at $\frac{1}{2}\lambda_c$ as $m \to \infty$, the same uniform argument cannot apply---indeed, it is always possible that the signs $\ba_j$ are all identical, in which case clearly it cannot be possible to recover $x^*$ any more effectively than in the one observation case.
Thus, to show that gTV norm minimization substantially improves on TV norm minimization, it is necessary to make some probabilistic assumptions about the data generating process of the $\ba_j$.

\section{Dual Certificate Construction}

Recall that we are interested in building a low-pass trigonometric polynomial $q: \TT \to \CC^m$ interpolating the points $(t_j, v_j)$ while remaining strictly inside the unit sphere in $\CC^m$ elsewhere.
The idea of the construction of \cite{fernandez2016super} when $m = 1$ is to build $q$ via kernel interpolation, and adjust it so that $q^\prime(t_j) = 0$ for each $j$, which one would hope might encourage the polynomial to remain in the unit sphere.
The naive extension of this to the $m$-dimensional is to require $\nabla q(t_j) = 0$ for each $t_j$.
However, numerical experiments show that the regime where this construction succeeds essentially does not depend on $m$, which is not surprising---after all, this is repeating the one-dimensional construction in each coordinate.
We propose a different dual certificate, which does succeed numerically in certifying reconstruction down to the Rayleigh limit as $m \to \infty$.

\subsection{Step 1: Relaxing Gradient Condition}

The first observation is that the condition $\nabla q(t_j) = 0$ is overkill for encouraging $q$ to remain in the unit sphere.
Intuitively, all that is really necessary is that $\nabla q(t_j)$ not have a component pointing out of the sphere, i.e.\ in the direction of $\bv_j$.
Thus, a reasonable condition is to require $\nabla q(t_j)$ and $\bv_j$, viewed as vectors, to be orthogonal.
However, it is not possible to express this constraint in complex arithmetic, so we must rewrite the dual certificate in terms of its real and complex parts (each of which is still low-pass).

Thus, we write $q_\ell(t) = q_\ell^R(t) + iq_\ell^I(t)$ for real-valued low-pass trigonometric polynomials $q_\ell^R$ and $q_\ell^I$.
We combine these into a single real-valued $2m$-dimensional trigonometric polynomial,
\begin{equation}
    \tilde{q}(t) = (q_1^R(t), q_1^I(t), \dots, q_m^R(t), q_m^I(t)) \in \RR^{2m}.
\end{equation}
Likewise writing $v_{j, \ell} = v_{j, \ell}^R + iv_{j, \ell}^I$ and $\tilde{\bv}_j = (v_{j, 1}^R, v_{j, 1}^I, \dots, v_{j, m}^R, v_{j, m}^I)$, we can express the interpolation conditions that the dual certificate $q$ must satisfy in terms of these new variables in essentially the same way: $\tilde{q}(t_j) = \tilde{\bv}_j$ for $j \in \{1, \dots, s\}$, and $\|\tilde{q}(t)\|_{\ell^2} < 1$ for all $t \notin T$.
We can also now formulate the conditions we will require of our interpolant.
\begin{enumerate}
\item[I1.] $\tilde{q}(t_j) = \tilde{\bv}_j$ for $j \in \{1, \dots, s\}$.
\item[I2.] $\la \nabla \tilde{q}(t_j), \tilde{\bv}_j \ra = 0$ for $j \in \{1, \dots, s\}$.
\end{enumerate}

\subsection{Step 2: Kernel Interpolation}

We now proceed just as in \cite{fernandez2016super}, viewing the problem constructed in the previous section as a purely real-valued one: fix a real-valued low-pass kernel $K$, and define
\[ \tilde{q}_{\ell}(t) = \sum_{j = 1}^s \alpha_{j, \ell}K(t - t_j) + \sum_{j = 1}^s \beta_{j, \ell} K^\prime(t - t_j) \]
for $\ell \in [2m]$ and coefficients $\alpha_{j, \ell}, \beta_{j, \ell} \in \RR$ to be determined.
We think of these as organized into vectors $\balpha, \bbeta \in \RR^{2sm}$ where the entries are written iterating first over $j$, then over $\ell$ (in ``row major'' order).
To concisely write conditions I1 and I2 from the previous part, we first make a small definition.
\begin{definition}
    For a function $K: \TT \to \RR$ and an ordered point set $T = \{t_1, \dots, t_s\} \subset \TT$, define $\bD(K, T) \in \RR^{s \times s}$ to have entries
    \begin{equation}
        (\bD(K, T))_{j, k} = K(t_j - t_k).
    \end{equation}
\end{definition}
We also define the vector $\tilde{\bv}$ to be the $\tilde{v}_{j, \ell}$ flattened also in row-major order.
Conditions I1 and I2 may then be written as the matrix equation
\[ \left[ \begin{array}{cc} \bm I_{2m} \otimes \bD(K, T) & \bm I_{2m} \otimes \bD(K^\prime, T) \\ (\one_s\bv^\top) \odot (\one_{2m}^\top \otimes \bD(K^\prime, T)) & (\one_s \bv^\top) \odot (\one_{2m}^\top \otimes \bD(K^{\prime\prime}, T)) \end{array}\right]\left[\begin{array}{c}\balpha \\ \bbeta \end{array}\right] = \left[\begin{array}{cc} \bv \\ \bm 0 \end{array}\right]. \]
We set some notation for the matrix on the left-hand side and the blocks corresponding to the set of equality constraints and gradient constraints:
\[ \bA = \left[\begin{array}{c} \bA_{0} \\ \bA_{\nabla} \end{array}\right] = \left[ \begin{array}{cc} \bm I_{2m} \otimes \bD(K, T) & \bm I_{2m} \otimes \bD(K^\prime, T) \\ (\one_s\bv^\top) \odot (\one_{2m}^\top \otimes \bD(K^\prime, T)) & (\one_s \bv^\top) \odot (\one_{2m}^\top \otimes \bD(K^{\prime\prime}, T)) \end{array}\right]. \]

\subsection{Step 3: Choosing Solution of Underconstrained System}

Note that the system described in the previous section has $4sm$ variables but only $2sm + s = (2m + 1)s$ constraints.
When the sign pattern is purely real, this reduces to $2s$ variables and $2s$ constraints, so there is (generically) a unique solution, as in the construction treated in \cite{fernandez2016super}.
In our more general setting, however, uniqueness typically fails, and so we must build a criterion for choosing a solution.

This extra freedom lets us further encourage $q(t)$ to remain inside the unit sphere.
In particular, a natural quantity to minimize is the $L^2$ norm of $q(t)$:
\begin{equation}
    \|q\|_{L^2(\TT)}^2 = \int_0^1 \|q(t)\|_{\ell^2}^2 dt.
\end{equation}
The added benefit of this choice is that it is a quadratic form in terms of the coefficients $\balpha, \bbeta$, so its minimization subject to linear constraints has a convenient closed form.
We now develop some notation to write down this quadratic form.
\begin{definition}
    For real-valued functions $K_1, K_2 \in L^2(\TT)$ and an ordered point set $T = \{t_1, \dots, t_s\}$, define $\bG(K_1, K_2, T) \in \RR^{s \times s}$ to have entries
    \begin{equation}
        (\bG(K_1, K_2, T))_{j, k} = \int_0^1 K_1(t - t_j)K_2(t - t_k)dt.
    \end{equation}
\end{definition}
Now, the $L^2$ norm of the polynomial $q(t)$ with real and imaginary parts as defined in \eqref{eq:interpolant-def-1} and \eqref{eq:interpolant-def-2} respectively is given by
\begin{align}
  \|q\|_{L^2(\TT)}^2
  &= \sum_{\ell = 1}^{2m} \sum_{j = 1}^s \sum_{k = 1}^s \bigg(\alpha_{\ell, j} \alpha_{\ell, k} \la K(\bullet - t_j), K(\bullet - t_k) \ra
    + \beta_{\ell, j} \beta_{\ell, k} \la K^\prime(\bullet - t_j), K^\prime(\bullet - t_k) \ra \nonumber \\
  &= \left[\begin{array}{c} \balpha \\ \bbeta \end{array}\right]^\top \left[\begin{array}{cc} \bm I_{2m} \otimes \bG(K, K, T) & \bm I_{2m} \otimes \bG(K, K^\prime, T) \\ \bm I_{2m} \otimes \bG(K^\prime, K, T) & \bm I_{2m} \otimes \bG(K^\prime, K^\prime, T) \end{array}\right]\left[\begin{array}{c} \balpha \\ \bbeta \end{array}\right].
\end{align}
The inner matrix may be further simplified if we set a notation for the Gram matrix of the $2s$ functions $K(\bullet - t_j)$ and $K^\prime(\bullet - t_j)$ in $L^2(\TT)$:
\begin{align}
  \bG_0 &= \left[\begin{array}{cc} \bG(K, K, T) & \bm \bG(K, K^\prime, T) \\  \bG(K^\prime, K, T) & \bG(K^\prime, K^\prime, T) \end{array}\right] \succeq 0, \\
  \bG &= \left[\begin{array}{cc} 1 & 1 \\ 1 & 1 \end{array}\right] \otimes \bm I_{2m} \otimes \bG_0 \succeq 0.
\end{align}
We may then concisely write down the full quadratic problem by which we will determine the coefficients $\balpha, \bbeta$ as follows:
\begin{align}
  \mathsf{minimize} \hs & \hs \left[\begin{array}{c} \balpha \\ \bbeta \end{array}\right]^\top \bG \left[\begin{array}{c} \balpha \\ \bbeta \end{array}\right] \\
  \mathsf{subject\ to} \hs & \hs \bA \left[\begin{array}{c} \balpha \\ \bbeta \end{array}\right] = \left[\begin{array}{c} \bv \\ \bm 0 \end{array}\right] \label{eq:opt-problem}
\end{align}

\subsection{Explicit Solution}

As mentioned previously, \eqref{eq:opt-problem} has a closed-form solution, which may be derived with a straightforward Lagrange multiplier calculation, similar to the usual derivation of linearly constrained least-squares (corresponding to the case $\bG = \bm I$).
In our setting, the coefficients $\balpha, \bbeta$ are given by
\[ \left[\begin{array}{c} \balpha \\ \bbeta \\ \bm \lambda \end{array}\right] = \left[ \begin{array}{cc} \bG & \bA^\top \\ \bA & \bm 0 \end{array}\right]^{-1}\left[ \begin{array}{c} \bm 0 \\ \bm v \\ \bm 0 \end{array}\right] \]
where $\bm \lambda \in \RR^{(2m + 1)s}$ are the Lagrange multipliers introduced in the calculation.
(The vectors on either side belong to $\RR^{(6m + 1)s}$.)
Moreover, we may explicitly invert this block matrix, and dispose of the Lagrange multipliers to obtain the simpler formula
\begin{equation}
    \left[\begin{array}{c} \balpha \\ \bbeta \end{array}\right] = \bG^{-1}\bA^\top(\bA \bG^{-1} \bA^\top)^{-1} \left[\begin{array}{c} \bv \\ \bm 0 \end{array}\right].
    \label{eq:opt-closed-form}
\end{equation}
Formula \eqref{eq:opt-closed-form} will be the beginning of our subsequent analysis; the remaining task is essentially just to obtain a detailed understanding of the structure of the matrix on the right-hand side.

\section{Numerical Evaluation}

In this section, we present some numerical results illustrating the performance of this dual certificate construction as the parameter $m$ increases.
These should be compared to Figures 3 and 9 of \cite{fernandez2016super} to observe that the dual certificate is feasible (and therefore certifies recovery) in basically all cases where the \textsf{gTV} convex program itself successfully achieves recovery.
First, in Figure~\ref{fig:real-complex}, we compare the case $m = 1$ with the case $m = 2$, which is the difference between real and complex sign patterns, and see that our dual certificate indeed is feasible more often in the latter case.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[scale=0.55]{../img/results-m=1.png}
    \end{center}
    \vspace{-0.5cm}
    \caption{\textbf{Real vs. Complex Amplitudes.} We show that our dual certificate construction performs better on i.i.d.\ random complex sign patterns (so that the sign pattern is uniform on the complex unit circle) than on i.i.d.\ real sign patterns (equal to $\pm 1$ with equal probability), showing the fraction of trials (out of 10) that the dual certificate is feasible in each case for fixed number of signals and minimum separation, and the difference between the fractions in the last panel.}
    \label{fig:real-complex}
\end{figure}

Second, in Figure~\ref{fig:higher-m}, we consider higher values of $m$, and note that the improvement continues, and the minimum separation at which the dual certificate stops being feasible for large support sets appears to approach the Rayleigh limit of $\frac{1}{2}\lambda_c$ as $m \to \infty$.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[scale=0.55]{../img/results-m=4.png}
        \HS \HS
        \includegraphics[scale=0.55]{../img/results-m=10.png}
    \end{center}
    \vspace{-0.5cm}
    \caption{\textbf{Increasing Dimension.} We show that the trend of improving feasibility performance as $m \to \infty$ continues for higher values of $m$, with the feasibility phase transition appearing to saturate around a minimum separation of $\frac{1}{2}\lambda_c$.}
    \label{fig:higher-m}
\end{figure}

\section{Proof of Dual Feasibility}

\nocite{*}
\bibliographystyle{unsrt}
\bibliography{main}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
