\documentclass[11pt]{article}

% Packages
\usepackage{enumitem}
\usepackage{amscd}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{bbold}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{color}
\usepackage{easybmat}
\usepackage{etex}
\usepackage{framed}
\usepackage[dvips,letterpaper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage{mathtools}
\usepackage{setspace}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage[capitalize]{cleveref}

%
% Commands
%

% Figures
\newcommand{\fig}[1]{(figure \ref{#1})}
\newcommand{\FIG}[1]{figure \ref{#1}}

\DeclareSymbolFont{bbold}{U}{bbold}{m}{n}
\DeclareSymbolFontAlphabet{\mathbbold}{bbold}

% Names
\newcommand{\Mobius}{M\"{o}bius}
\newcommand{\Holder}{H\"{o}lder}
\newcommand{\Rouche}{Rouch\'{e}}
\newcommand{\Ito}{It\={o}}
\newcommand{\Kondo}{Kond\^{o}}
\newcommand{\Levy}{L\'{e}vy}
\newcommand{\Cramer}{Cram\'{e}r}
\newcommand{\Godel}{G\"{o}del}
\newcommand{\Carath}{Carath\'{e}odory}
\newcommand{\Caratheodory}{Carath\'{e}odory}
\newcommand{\Hopital}{H\^{o}pital}

% Random
\renewcommand{\bar}{\overline}
\newcommand{\lvec}{\overrightarrow}
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}

% Disjoint union
\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand{\charfusion}[3][\mathord]{
    #1{\ifx#1\mathop\vphantom{#2}\fi
        \mathpalette\mov@rlay{#2\cr#3}
      }
    \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother

\newcommand{\cupdot}{\charfusion[\mathbin]{\cup}{\cdot}}
\newcommand{\bigcupdot}{\charfusion[\mathop]{\bigcup}{\cdot}}

% Blackboard bold
\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\DD}{\mathbb{D}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\FF}{\mathbb{F}}
\renewcommand{\SS}{\mathbb{S}}
\newcommand{\Fp}{\FF_p}
\newcommand{\TrivGp}{\mathbbold{1}}
\newcommand{\One}{\mathbbold{1}}

\newcommand{\RP}{\RR\mathrm{P}}
\newcommand{\CP}{\CC\mathrm{P}}

% Vector bold
\newcommand{\nn}{\bm{n}}
\newcommand{\vv}{\bm{v}}
\newcommand{\ww}{\bm{w}}
\newcommand{\xx}{\bm{x}}
\newcommand{\yy}{\bm{y}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\one}{\bm{1}}

% Other fonts
\newcommand{\fp}{\mathfrak{p}}
\newcommand{\fq}{\mathfrak{q}}
\newcommand{\fg}{\mathfrak{g}}
\newcommand{\fh}{\mathfrak{h}}
\newcommand{\fa}{\mathfrak{a}}
\newcommand{\fb}{\mathfrak{b}}
\newcommand{\fc}{\mathfrak{c}}
\newcommand{\fm}{\mathfrak{m}}
\renewcommand{\sl}{\mathfrak{sl}}
\newcommand{\so}{\mathfrak{so}}
\newcommand{\gl}{\mathfrak{gl}}
\renewcommand{\sp}{\mathfrak{sp}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\sB}{\mathcal{B}}
\newcommand{\sC}{\mathcal{C}}
\newcommand{\sD}{\mathcal{D}}
\newcommand{\sE}{\mathcal{E}}
\newcommand{\sF}{\mathcal{F}}
\newcommand{\sG}{\mathcal{G}}
\newcommand{\sH}{\mathcal{H}}
\newcommand{\sI}{\mathcal{I}}
\newcommand{\sL}{\mathcal{L}}
\newcommand{\sM}{\mathcal{M}}
\newcommand{\sN}{\mathcal{N}}
\newcommand{\sO}{\mathcal{O}}
\newcommand{\sP}{\mathcal{P}}
\newcommand{\sR}{\mathcal{R}}
\newcommand{\sS}{\mathcal{S}}
\newcommand{\sT}{\mathcal{T}}
\newcommand{\sU}{\mathcal{U}}
\newcommand{\sV}{\mathcal{V}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}

% Spacing
\newcommand\ThmBr{%
    \@ifstar{\item[\vbox{\null}]}{%
      \begingroup % keep changes local
      \setlength\itemsep{0pt}%
      \setlength\parsep{0pt}%
       \item[\vbox{\null}]%
      \endgroup%
     }}
\newcommand{\br}{\vspace{1pc}}
\newcommand{\BR}{\vspace{2pc}}
\newcommand{\picspace}{\vspace{13pc}}
\newcommand{\hs}{\hspace{1mm}}
\newcommand{\HS}{\hspace{3.5mm}}
\newcommand{\hr}{
  \begin{center}
    \line(1,0){250}
  \end{center}
}
\newcommand{\hrs}{
  \begin{center}
    \line(1,0){150}
  \end{center}
}

% Plain text
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\newcommand{\disc}{\mathrm{disc}}
\newcommand{\Ann}{\mathrm{Ann}}
\newcommand{\Ass}{\mathrm{Ass}}
\newcommand{\Soc}{\mathrm{Soc}}
\newcommand{\Supp}{\mathrm{Supp}}
\newcommand{\Spec}{\mathrm{Spec}}
\newcommand{\maxSpec}{\mathrm{maxSpec}}

\newcommand{\N}{\mathrm{N}}
\newcommand{\Tr}{\mathrm{Tr}}

% Functors
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\Der}{\mathrm{Der}}
\newcommand{\End}{\mathrm{End}}
\newcommand{\Ind}{\mathrm{Ind}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Gal}{\mathrm{Gal}}
\newcommand{\Sym}{\mathrm{Sym}}
\newcommand{\Rad}{\mathrm{Rad}}
\newcommand{\Id}{\mathrm{Id}}
\newcommand{\Ad}{\mathrm{Ad}}
\newcommand{\ad}{\mathrm{ad}}
\newcommand{\Pow}{\mathrm{Pow}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\img}{\mathrm{img}}
\newcommand{\sgn}{\mathrm{sgn}}

\newcommand{\ch}{\mathrm{char}}
\newcommand{\Res}{\mathrm{Res}}
\newcommand{\ord}{\mathrm{ord}}
\newcommand{\cont}{\mathrm{cont}}
\newcommand{\ab}{\mathrm{ab}}
\newcommand{\Orb}{\mathrm{Orb}}
\newcommand{\Syl}{\mathrm{Syl}}
\newcommand{\Irr}{\mathrm{Irr}}
\newcommand{\Frac}{\mathrm{Frac}}
\newcommand{\sep}{\mathrm{sep}}
\newcommand{\per}{\mathrm{per}}

% Groups
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\PGL}{\mathrm{PGL}}
\newcommand{\PSL}{\mathrm{PSL}}
\newcommand{\SL}{\mathrm{SL}}
\newcommand{\oO}{\mathrm{O}}
\newcommand{\SO}{\mathrm{SO}}
\newcommand{\PSO}{\mathrm{PSO}}
\newcommand{\Sp}{\mathrm{Sp}}
\newcommand{\PSp}{\mathrm{PSp}}
\newcommand{\U}{\mathrm{U}}
\newcommand{\SU}{\mathrm{SU}}
\newcommand{\PSU}{\mathrm{PSU}}

% Parentheses
\newcommand{\lgndr}[2]{\ensuremath{\left(\frac{#1}{#2}\right)}}

% Mappings
\newcommand{\iso}{\cong}
\newcommand{\eqdf}{\stackrel{\mathrm{df}}{=}}
\newcommand{\eqd}{\stackrel{\mathrm{d}}{=}}
\newcommand{\eqqu}{\stackrel{\mathrm{?}}{=}}
\newcommand{\xto}{\xrightarrow}
\newcommand{\dto}{\Rightarrow}
\newcommand{\into}{\hookrightarrow}
\newcommand{\xinto}{\xhookrightarrow}
\newcommand{\onto}{\twoheadrightarrow}
\newcommand{\xonto}{xtwoheadrightarrow}
\newcommand{\isoto}{\xto{\sim}}
\newcommand{\upto}{\nearrow}
\newcommand{\downto}{\searrow}

% Convenience
\newcommand{\Implies}{\ensuremath{\Rightarrow}}
\newcommand{\ImpliedBy}{\ensuremath{\Leftarrow}}
\newcommand{\Iff}{\ensuremath{\Leftrightarrow}}

\newcommand{\Pfright}{\ensuremath{(\Rightarrow):\hs}}
\newcommand{\Pfleft}{\ensuremath{(\Leftarrow):\hs}}

\newcommand{\sm}{\ensuremath{\setminus}}

\newcommand{\tab}[1]{(table \ref{#1})}
\newcommand{\TAB}[1]{table \ref{#1}}

\newcommand{\precode}[1]{\textbf{\footnotesize #1}}
\newcommand{\code}[1]{\texttt{\footnotesize #1}}

\newcommand{\sectionline}{
  \nointerlineskip \vspace{\baselineskip}
  \hspace{\fill}\rule{0.35\linewidth}{.7pt}\hspace{\fill}
  \par\nointerlineskip \vspace{\baselineskip}
}


%
% Misc
%

\parskip0em
\linespread{1.05}
\widowpenalty10000
\clubpenalty10000


\newcommand{\HC}{\mathsf{HC}}
\newcommand{\CHC}{\mathsf{CHC}}
\newcommand{\SK}{\mathsf{SK}}
\newcommand{\SDP}{\mathsf{SDP}}
\newcommand{\SOS}{\mathsf{SOS}}
\newcommand{\PE}{\mathsf{PE}}
\newcommand{\PS}{\mathsf{PS}}
\newcommand{\tEE}{\tilde{\mathbb{E}}}
\newcommand{\tCov}{\widetilde{\mathrm{Cov}}}
\newcommand{\sfP}{\mathsf{P}}
\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\diag}{\mathsf{diag}}
\newcommand{\eq}{\mathsf{eq}}
\newcommand{\der}{\mathsf{der}}
\newcommand{\vrad}{\mathrm{vrad}}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\rkeff}{\mathrm{rk}_{\mathrm{eff}}}
\newcommand{\GOE}{\mathsf{GOE}}
\newcommand{\ssG}{\mathsf{G}}
\newcommand{\balpha}{\bm \alpha}
\newcommand{\blambda}{\bm \lambda}
\newcommand{\bbeta}{\bm \beta}
\newcommand{\bA}{\bm A}
\newcommand{\bB}{\bm B}
\newcommand{\bC}{\bm C}
\newcommand{\bD}{\bm D}
\newcommand{\bF}{\bm F}
\newcommand{\bG}{\bm G}
\newcommand{\bH}{\bm H}
\newcommand{\bK}{\bm K}
\newcommand{\bM}{\bm M}
\newcommand{\bP}{\bm P}
\newcommand{\bQ}{\bm Q}
\newcommand{\bR}{\bm R}
\newcommand{\bS}{\bm S}
\newcommand{\bV}{\bm V}
\newcommand{\bW}{\bm W}
\newcommand{\bX}{\bm X}
\newcommand{\bY}{\bm Y}
\newcommand{\ba}{\bm a}
\newcommand{\bb}{\bm b}
\newcommand{\bc}{\bm c}
\newcommand{\be}{\bm e}
\newcommand{\bg}{\bm g}
\newcommand{\bv}{\bm v}
\newcommand{\bw}{\bm w}
\newcommand{\bx}{\bm x}
\newcommand{\bq}{\bm q}
\newcommand{\bkappa}{\bm \kappa}
\newcommand{\tp}{\mathsf{3pt}}
\newcommand{\od}{\mathsf{1D}}

\newcommand{\bmat}[2]{
	\begin{bmatrix*}[#1]
		#2
	\end{bmatrix*}
}

\DeclareRobustCommand{\bmrob}[1]{\bm{#1}}
\pdfstringdefDisableCommands{%
  \renewcommand{\bmrob}[1]{#1}%
}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{question}{Question}
\newtheorem{task}{Task}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

\newcounter{para}
\newcommand\mypara[1]{\paragraph{\refstepcounter{para}#1}}

\title{Super-Resolution of Point Sources Down to the Rayleigh Limit from Multiple Observations}
\author{}

\begin{document}

\maketitle

\noindent

\section{Formal Setup}

\subsection{Single Observation Problem Statement}

Suppose we observe a true signal whose representation in the physical domain is
\begin{equation}
    x^*(t) = \sum_{j = 1}^s a_j \delta_{t_j}(t),
\end{equation}
for $t \in \TT$ where $\TT = \RR / \ZZ$, which we will usually think of as the unit interval $[0, 1]$ with its endpoints identified, and $T = \{t_j\}_{j = 1}^s \subset \TT$ a discrete support set.
In the Fourier domain, this signal takes the form
\begin{equation}
    \widehat{x^*}(k) = \sum_{j = 1}^s a_j \exp(-2\pi i k t_j)
\end{equation}
for $k \in \ZZ$.

Now, we are interested in recovering this true signal $x$ from an observation that suffers from low resolution, which we represent as convolution of $x(t)$ with a point-spread function (PSF) denoted by $\phi(t)$.
We then observe the signal
\begin{equation}
    (\phi * x^*)(t) = \sum_{j = 1}^sa_j \phi(t - t_j).
\end{equation}
In the simplest case, $\phi$ is a Dirichlet kernel with cutoff frequency $f_c$, in which case the Fourier transform of the above is simply the truncation of the Fourier transform of $x^*$.
Denoting by $y$ the signal we observe in the Fourier domain, we have
\begin{equation}
    y(k) = \left(\sum_{j = 1}^s a_j \exp(-2\pi i k t_j)\right)\One\{|k| \leq f_c \}.
\end{equation}
We may then think of the data we observe as simply the $2f_c + 1$ values $y(-f_c), \dots, y(f_c)$.
We write $\sF_n$ for the sensing operator mapping $x^*$ to these $n$ Fourier coefficients.
A popular technique for solving this problem is \emph{total variation minimization}, which attempts to recover $x^*$ by solving the convex problem
\begin{equation}
\begin{aligned}
  \text{minimize} &\HS \|x\|_{\mathsf{TV}} \\
  \text{subject to} &\HS \sF_nx = y.
\end{aligned}
\end{equation}

\subsection{Extension to Multiple Observations}

We now consider a generalization of the problem presented in the previous section, where we make \emph{several} observations signals sharing the support $T = \{t_1, \dots, t_s\}$ of $x^*$, but having varying amplitudes $a_j$.
To formalize this, our true signal is now
\begin{equation}
    x^*_\ell(t) = \sum_{j = 1}^s a_{\ell, j} \delta_{t_j} \text{ for } \ell \in \{1, \dots, m\},
\end{equation}
and our observations are $y_{\ell, k} = (\sF_n x^*_\ell)_{k}$ for $k \in \{-f_c, \dots, f_c\}$ and $\ell \in \{1, \dots, m\}$.
We think of the $a_{\ell, j}$ as organized into vectors $\ba_{j} \in \RR^m$ for $j \in \{1, \dots, s\}$, and the  $y_{\ell, k}$ as organized into a matrix $\bY \in \CC^{m \times n}$.
The \emph{group total variation minimization} is the natural extension of the previous convex problem to this setting, where we solve
\begin{equation}
\begin{aligned}
  \text{minimize} &\HS \|x\|_{\mathsf{gTV}} \\
  \text{subject to} &\HS \left[\begin{array}{cccc}\sF_nx_1 & \sF_n x_2 \cdots \sF_n x_m \end{array}\right]^\top = \bY.
\end{aligned}
\end{equation}

\subsection{Dual Certificates}

In this section, we briefly describe the results of applying Lagrangian duality to \textsf{TV} and \textsf{gTV} norm minimization, the main tool for theoretical analysis of the performance of these algorithms.

\begin{definition}
    Let $\mu^0 \subset \CC$ denote the complex unit circle.
    A \emph{sign pattern} on a set is an assignment of points of $\mu^0$ to each point of the set.
\end{definition}

\begin{definition}
    For a sign pattern $v \in (\mu^0)^T$, a low-pass trigonometric polynomial $q: \TT \to \CC$,
    \begin{equation}
        q(t) = \sum_{k = -f_c}^{f_c} c_k e^{2\pi i k t},
    \end{equation}
    is a \emph{single-observation dual certificate} for $v$ if $q(t_j) = v_j$ for $t_j \in T$ and $|q(t)| < 1$ for $t \notin T$.
\end{definition}
\begin{proposition}[\textsf{TV} Norm Minimization Duality]
If a dual certificate for the sign pattern $v_j = a_j / |a_j|$ exists, then $x^*$ is the unique solution of \textsf{TV} minimization for the super-resolution problem.
\end{proposition}
Therefore, to prove the effectiveness of \textsf{TV} minimization it suffices to show that a dual certificate exists under some conditions on $T$.
The main result of \cite{fernandez2016super} establishes that this is true under a minimum separation condition.
\begin{definition}
    The \emph{minimum separation} of a set $T \subset \TT$ is
    \begin{equation}
        \Delta(T) = \min_{\substack{s, t \in T \\ s \neq t}} | s - t |,
    \end{equation}
    where we measure the wrap-around distance as on a circle.
\end{definition}

\begin{theorem}[Proposition 2.3 of \cite{fernandez2016super}]
    If $\Delta(T) \geq 1.26\lambda_c$ and $f_c \geq 10^3$, then a dual certificate exists for any sign pattern on $T$.
    \label{thm:single-obs-recovery}
\end{theorem}

As also observed in \cite{fernandez2016super}, the same ideas extend to the multiple observation case in a straightforward fashion.
\begin{definition}
    Let $\mu^{m - 1} \subset \CC^m$ denote the $m$-dimensional complex unit sphere.
    A \emph{$m$-dimensional sign pattern} on a set is an assignment of points of $\mu^{m - 1}$ to each point of the set.
\end{definition}
\begin{definition}
    For a sign pattern $\bv \in (\mu^{m - 1})^T$, a low-pass trigonometric polynomial $q: \TT \to \CC^m$ given by $q(t) = (q_1(t), \dots, q_m(t))$ and
    \begin{equation}
        q_\ell(t) = \sum_{k = -f_c}^{f_c} c_k \exp(2\pi i k t) \text{ for } \ell \in \{1, \dots, m\}
    \end{equation}
    is an \emph{$m$-observation dual certificate} for $v$ if $q(t_j) = \bv_j$ for $t_j \in T$, and $\|q(t)\|_{\ell^2} < 1$ for $t \notin T$.
\end{definition}
\begin{proposition}[\textsf{gTV} Norm Minimization Duality]
    If a dual certificate for the sign pattern $\bv_j = \ba_{j} / \|\ba_{j}\|_{\ell^2}$ exists, then $x^*$ is the unique solution of \textsf{gTV} minimization for the super-resolution problem.
\end{proposition}
However, while numerical experiments in \cite{fernandez2016super} suggest that when the amplitude vectors $\ba_j$ are taken randomly then recovery is possible down to a lower critical minimum separation which accumulates at $\frac{1}{2}\lambda_c$ as $m \to \infty$, the same uniform argument cannot apply---indeed, it is always possible that the amplitudes $\ba_j$ are all identical, in which case clearly it cannot be possible to recover $x^*$ any more effectively than in the one observation case.
Thus, to show that \textsf{gTV} norm minimization substantially improves on \textsf{TV} norm minimization, it is necessary to make some incoherence assumptions about the data generating process of the $\ba_j$.
One simple such assumption that captures the idea of these points being in ``general position'' is to assume that the signs $\bv_j = \ba_j / \|\ba_j\|_{\ell^2}$ are i.i.d.\ isotropically random unit vectors.

\section{Dual Certificate Construction}

Recall that we are interested in building a low-pass trigonometric polynomial $q: \TT \to \CC^m$ interpolating the points $(t_j, \bv_j)$ while remaining strictly inside the unit sphere in $\CC^m$ elsewhere.
The general idea of such constructions is usually to choose some \emph{interpolation basis} of polynomials, and build $q$ as a linear combination of them under some constraints.
We first describe a simple approach to choosing interpolation bases based on constraints and a variational heuristic, that justifies several existing techniques and motivates our own.

\subsection{Equivalence of $L^2$ Norm Minimization and Interpolation}

Minimizing $\|q\|_{L^2([0, 1])}$ under some interpolation constraints is equivalent to interpolating with a restricted basis, the basis arising from the nature of the constraints.
This general principle can be implemented in specific cases with simple Lagrange multiplier calculations.
Consider, for instance, the problem
\begin{equation}
\begin{aligned}
  \text{minimize} &\HS \|q\|_{L^2([0, 1])} \\
  \text{subject to} &\HS q(t_j) = \bv_j \HS \forall j \in [s]
\end{aligned}
\end{equation}
Let us write
\begin{equation}
    q_{\bc}(t) = \sum_{f = -f_c}^{f_c} c_f \exp(2\pi i ft),
\end{equation}
and define the matrix $\bF_T \in \CC^{s \times (2f_c + 1)}$ by $(\bF_T)_{jf} = \exp(2\pi i ft_j)$, with $-f_c \leq f \leq f_c$.
Then, the calculation of the values of $q_{\bc}$ on $T$ is a matrix-vector multiplication,
\begin{equation}
    \left[\begin{array}{c} q_{\bc}(t_1) \\ \vdots \\ q_{\bc}(t_s) \end{array}\right] = \bF_T \bc,
\end{equation}
and $\|q_{\bc}\|_{L^2([0, 1])} = \|\bc\|_{2}$.
Thus, the above problem is equivalently
\begin{equation}
\begin{aligned}
  \text{minimize} &\HS \|\bc\|_{2} \\
  \text{subject to} &\HS \bF_T\bc = \bv
\end{aligned}
\end{equation}
We write a Lagrangian $L(\bc, \balpha) = \frac{1}{2}\|\bc\|_2^2 - \la \bm \alpha, \bF_T\bc - \bv \ra$, and upon differentiating and solving the first-order optimality condition obtain $\bc = \bF_T^*\bm \alpha$.
Generally, we have
\begin{equation}
    q_{\bF_T^* \bm \alpha}(t) = \sum_{f = -f_c}^{f_c} \left(\sum_{j = 1}^s \exp(-2\pi i ft_j)\alpha_j\right)\exp(2\pi i f t) = \sum_{j = 1}^s \alpha_jK(t - t_j)
\end{equation}
where $K$ is the Dirichlet kernel.
We therefore have found the formal equivalence:

\noindent
\begin{minipage}{.48\linewidth}
    \vspace{0.5cm}
\begin{equation*}
\begin{aligned}
  \text{minimize} &\HS \|q\|_{L^2([0, 1])} \\
  \text{subject to} &\HS q(t_j) = \bv_j \HS \forall j \in [s]
\end{aligned}
\end{equation*}
\end{minipage}
\begin{minipage}{.04\linewidth}
    \begin{equation*}
        \leftrightarrow
    \end{equation*}
\end{minipage}
\begin{minipage}{.48\linewidth}
\begin{equation}
\begin{aligned}
  \text{find} &\HS q(t) = \textstyle\sum_{j = 1}^s \alpha_jK(t - t_j) \\
  \text{subject to} &\HS q(t_j) = \bv_j \HS \forall j \in [s]
\end{aligned}
\end{equation}
\end{minipage}
\vspace{1em}

In the same way, one obtains a justification for the popular trick for allowing one to force the derivative of $q$ to be zero at each $t_j$ by adding terms $\beta_jK^\prime(t - t_j)$ to the sum above: an analogous calculation shows

\noindent
\begin{minipage}{.48\linewidth}
    \vspace{0.7cm}
    \begin{equation*}
\begin{aligned}
  \text{minimize} &\HS \|q\|_{L^2([0, 1])} \\ \\
  \text{subject to} &\HS q(t_j) = v_j \HS \forall j \in [s] \\
                  &\HS q^\prime(t_j) = 0 \HS \forall j \in [s]
              \end{aligned}
          \end{equation*}
\end{minipage}
\begin{minipage}{.04\linewidth}
    \[ \leftrightarrow \]
\end{minipage}
\begin{minipage}{.48\linewidth}
    \begin{equation}
\begin{aligned}
  \text{find} &\HS q(t) = \textstyle\sum_{j = 1}^s \alpha_jK(t - t_j) \hs + \\
  &\HS\HS\HS\HS \textstyle\sum_{j = 1}^s\beta_jK^\prime(t - t_j) \\
  \text{subject to} &\HS q(t_j) = v_j \HS \forall j \in [s] \\
                  &\HS q^\prime(t_j) = 0 \HS \forall j \in [s]
              \end{aligned}
          \end{equation}
\end{minipage}
\vspace{1em}

\subsection{Relaxing the Derivative Condition}

As mentioned before, in the multidimensional setting, we do not expect setting the derivative to equal zero at each interpolation point will allow us to lower the minimum separation threshold substantially beyond where the analogous one-dimensional dual certificate is feasible.
Instead, we propose a new construction where this condition is relaxed.
Rather than setting the derivative at each interpolation point to equal zero, we only constrain it to the tangent plane to the complex unit sphere at that interpolation point.
[TODO: something about not dividing real from complex for clarity though this helps in numerics].
Following the ideas in the previous section, we propose as a dual certificate the polynomial solving the following optimization problem:
\begin{equation}
    \tag{L2-MIN-FULL}
    \begin{aligned}
  \text{minimize} &\HS \|q\|_{L^2([0, 1])} \\
  \text{subject to} &\HS q(t_j) = \bv_j \HS \forall j \in [s] \\
                  &\HS \la q^\prime(t_j), \bv_j \ra = 0 \HS \forall j \in [s]
              \end{aligned}
          \end{equation}
Performing the translation to an interpolation problem as before gives the following:
\begin{equation}
    \tag{INT-FULL}
    \begin{aligned}
  \text{find} &\HS q_k(t) = \sum_{j = 1}^s \alpha_{j, k} K(t - t_j) + \sum_{j = 1}^s \beta_jV_{j, k} K^\prime(t - t_j) \\
  \text{subject to} &\HS q_k(t_j) = V_{j, k} \HS \forall j \in [s], k \in [m] \\
  &\HS \la q^\prime(t_j), \bv_j \ra = 0 \HS \forall j \in [s]
\end{aligned}
\end{equation}
where we organize the sign pattern vectors $\bv_j$ as the rows of the matrix $\bV \in \CC^{s \times m}$.
Note that if we instead took the constraint $q^\prime(t_j) = \bm 0$, then the second term would involve terms $\beta_{j, k}K^\prime(t - t_j)$ in coordinate $k$, but here we are only allowed \emph{one} set of coefficients $\beta_j$ to use across all coordinates.
This, along with the randomness of the $V_{j, k}$, essentially ensures that the second term cannot often be large: a single vector $\bm \beta$ cannot align with many weakly dependent random vectors $\bV_{\bullet, k}$ at once.

\subsection{Explicit Solution}

Next, we derive concise formulae solving for the coefficients $\bm \alpha$, $\bm \beta$ (when this is possible, which will later need some justification).
Let $\bK_r = (\frac{d^r}{dt^r}K(t_i - t_j))_{i, j = 1}^s$, then we may write
\begin{align}
  (q_k(t_j))_{j \in [s], k \in [m]} &= \bK_0 \balpha + \bK_1\diag(\bbeta) \bV \\
  (q_k^\prime(t_j))_{j \in [s], k \in [m]} &= \bK_1 \balpha + \bK_2\diag(\bbeta) \bV
\end{align}
Thus, the interpolation equations may be written
\begin{align}
  \bK_0 \balpha + \bK_1\diag(\bbeta) \bV &= \bV \\
  \diag((\bK_1 \balpha + \bK_2\diag(\bbeta) \bV)\bV^*) &= \bm 0
\end{align}
From the first equation we obtain
\begin{equation}
    \balpha = \bK_0^{-1}(\bm I_s - \bK_1 \diag(\bbeta))\bV,
\end{equation}
and substituting this into the second obtain
\begin{equation}
    \bbeta = \frac{\diag(\bV\bV^*\bK_0^{-1}\bK_1)}{\diag(\bV\bV^*\bK_2) - \diag(\bV\bV^*\bK_1\bK_0^{-1}\bK_1)},
\end{equation}
where division in the latter is coordinatewise, and these formulae hold provided $\bK_0$ is invertible and $\diag(\bV\bV^*(\bK_2 - \bK_1\bK_0^{-1}\bK_1))$ is nowhere zero.

\section{Special Case: Orthogonal Signs}

Consider the special case where the $\bv_j$ are orthogonal, and for the sake of simplicity $s = m$, so that they form a complete basis.
Then, without loss of generality we further consider the case where the $\bv_j$ are rotated to equal the standard basis, so that $\bV = \bm I_s$.
One may then simplify the equations from before, but it is even easier to note directly that the equations governing each polynomial coordinate $q_k(t)$ become decoupled: we are now interested in the interpolation problem
\begin{equation}
    \tag{INT-ORTH}
    \begin{aligned}
  \text{find} &\HS q_k(t) = \sum_{j = 1}^s \alpha_{j, k} K(t - t_j) + \beta_kK^\prime(t - t_k) \\
  \text{subject to} &\HS q_k(t_j) = \delta_{j, k} \HS \forall j \in [s], k \in [m] \\
  &\HS q_k^\prime(t_k) = 0 \HS \forall k \in [m]
\end{aligned}
\end{equation}
Note that this decomposes into $s$ independent interpolation problems, each of the form
\begin{equation}
    \tag{INT-1D}
    \begin{aligned}
  \text{find} &\HS q(t) = \sum_{j = 1}^{s} \alpha_j K(t - t_j) + \beta K^\prime(t - t_1) \\
  \text{subject to}
              &\HS q(t_j) = \delta_{1,j} \HS \forall j \in [s] \\
              &\HS q^\prime(t_1) = 0
          \end{aligned}
\end{equation}

\subsection{Explicit Solution}

From the second constraint we may directly solve
\[ \beta_k = -\frac{1}{K^{\prime\prime}(0)}\sum_{j = 1}^s \alpha_{j, k}K^\prime(t_k - t_j), \]
and therefore
\[ q_k(t) = \sum_{j = 1}^s\alpha_{j, k}\left(K(t - t_j) - \frac{K^\prime(t_k - t_j)K^\prime(t - t_k)}{K^{\prime\prime}(0)}\right). \]
We define the matrix corresponding to this linear system, $\bm M_k$, by
\[ (\bM_k)_{ij} = K(t_i - t_j) + \frac{K^\prime(t_k - t_j)K^\prime(t_k - t_i)}{K^{\prime\prime}(0)}, \]
so that, writing $\balpha_k = \balpha_{\bullet, k}$, we find $\balpha_k = \bm M^{-1}_k \bm e_k$.
We may further decompose
\[ \bm M_k = \bK_0 + \bm \Delta_k, \]
where
\[ (\bm \Delta_k)_{ij} = \frac{K^\prime(t_k - t_j)K^\prime(t_k - t_i)}{K^{\prime\prime}(0)}. \]
This matrix is rank one:
\begin{align}
  \bm \Delta_k &= \frac{1}{K^{\prime\prime}(0)} \bm \bkappa_{1, k} \bm \bkappa_{1, k}^\top \\
  (\bkappa_{1, k})_i &= K^\prime(t_k - t_i)
\end{align}
Therefore, $\bM_k^{-1}$ may be computed by the Sherman-Morrison formula:
\[ \bM_k^{-1} = \bK_0^{-1} - \frac{1}{K^{\prime\prime}(0) + \bkappa_{1, k}^{\top} \bK_0^{-1} \bkappa_{1, k}}\bK_0^{-1}\bkappa_{1, k} \bkappa_{1, k}^\top \bK_0^{-1}, \]
and we obtain the formula
\[ \balpha_k = \bM_k^{-1} \be_k = \bK_0^{-1}\be_k - \frac{\bkappa_{1, k}^\top \bK_0^{-1}\be_k}{K^{\prime\prime}(0) + \bkappa_{1, k}^\top\bK_0^{-1} \bkappa_{1, k}}\bK_0^{-1}\bkappa_{1, k}, \]
expressing $\balpha_k$ as a perturbation of of $\bK_0^{-1}\be_k$, which is what we would obtain if we were simply doing Dirichlet kernel interpolation on the sign pattern $\be_k$.
Indeed, if only the first term were present, we would simply have $\bm \alpha \approx \bK_0^{-1}$.

\subsection{Approximation of $K_0^{-1}$}
We build an approximation of $\bK_0^{-1}$ column by column.
Without loss of generality, suppose we are interested in approximating the second column of $\bK_0^{-1}$.
Our approximation will consist in estimating the first, second, and third entries, and claiming that the remaining entries must be small.
Let us write
\begin{equation}
    \bK_0 = \left[ \begin{array}{cc} \bA & \bB \\ \bB^\top & \bC \end{array} \right],
\end{equation}
with $\bA \in \RR^{3 \times 3}$, $\bB \in \RR^{3 \times (s - 3)}$, and $\bC \in \RR^{(s - 3) \times (s - 3)}$.
Inverting the block matrix, we find that the first three columns of the inverse are given by
\begin{equation}
    \bK_0^{-1} = \left[ \begin{array}{cc} \bA^{-1} + \bA^{-1}\bB(\bK_0 / \bA)^{-1}\bB^\top \bA^{-1} & \cdots \\ -(\bK_0 / \bA)^{-1}\bB^\top \bA^{-1} & \cdots \end{array} \right],
\end{equation}
where $\bK_0 / \bA = \bC - \bB^\top \bA^{-1} \bB$ is the Schur complement.
We will argue that the terms besides $\bA^{-1}$ are small by showing that $\bB$ is small and the other terms are not too large.
We have
\begin{align}
  \|\bB\|_\infty = \max_{i \in \{1, 2, 3\}} \sum_{j = 1}^{n - 3} |B_{ij}|
\end{align}

\subsection{Three-Point Approximation}

We now introduce a means of approximating the solution to (INT-1D).
The idea is to solve a truncated version of the same problem in closed form, and hope that the resulting solution is close to the true solution of (INT-1D).
The truncated problem is as follows, obtained by simply ignoring all constraints on points other than $t_s, t_1, t_2$ (the three-point neighborhood surrounding $t_1$, where the interpolant must equal 1).
Here, it is convenient to assume (without loss of generality) that $t_1 = 0$, and to rename $t_s = -\Delta_-$ and $t_2 = \Delta_{+}$ where $\Delta_{\pm} > 0$, for the sake of more symmetrical notation.

\begin{equation}
    \tag{INT-1D-TRUNC}
    \begin{aligned}
        \text{find} &\HS q(t) = \alpha_0 K(t) + \alpha_+K(t - \Delta_+) + \alpha_-K(t + \Delta_-) + \beta K^\prime(t) \\
        \text{subject to}
        &\HS q(0) = 1 \\
        &\HS q(\Delta_+) = q(-\Delta_-) = 0 \\
        &\HS q^\prime(0) = 0
    \end{aligned}
\end{equation}
The coefficients of the solution solve the following linear system of equations in four variables:
\begin{equation}
    \left[
        \begin{array}{cccc}
          K(0) & K(\Delta_+) & K(\Delta_-) & K^{\prime}(0) \\
          K(\Delta_+) & K(0) & K(\Delta_+ + \Delta_-) & K^\prime(\Delta_+) \\
          K(\Delta_-) & K(\Delta_+ + \Delta_-) & K(0) & -K^\prime(\Delta_-) \\
          K^\prime(0) & K^\prime(\Delta_+) & -K^\prime(\Delta_-) & K^{\prime\prime}(0)
        \end{array}
    \right]
    \left[\begin{array}{c} \alpha_0 \\ \alpha_+ \\ \alpha_- \\ \beta \end{array}\right]
    =
    \left[\begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array}\right].
\end{equation}
We may solve this in closed form, obtaining an explicit (if complicated) function that gives the solution:
\begin{equation}
    q^{\tp}(\Delta_+, \Delta_-; t) : [0, 1] \times [0, 1] \times \TT \to \RR
\end{equation}
Let us similarly set some notation for the solution of (INT-1D):
\begin{equation}
    q^{\od}(T; t): [0, 1]^s \times \TT \to \RR,
\end{equation}
then our main hypothesis is that the following approximation is sound: suppose $T = \{0 = t_1, t_2, \dots, t_s\}$ with these points ordered along $\TT$, then
\begin{equation}
    q^{\od}(\{0 = t_1, t_2, \dots, t_s\}; \bullet) \approx q^{\tp}(|t_2|, |t_s|; \bullet).
\end{equation}


\section{Further Special Case: Equally Spaced Points}

Consider further a special case where the $t_j$ are all equally spaced in $\TT$, so that $\Delta = 1/s$, and every pair of $t_j, t_{j + 1}$ achieves this spacing exactly in the wraparound metric.
The point of this simplification is to induce some extra structure in the matrix $\bK_0$ to further ease the formal analysis performed in the previous section.
If we do this, then if we have $1/s = \Delta = \gamma\lambda_c = \gamma / f_c$, we must have $f_c = \gamma s$ is an integer.

Indeed, in this case we have $t_i - t_j = (i - j)\Delta$ (with equality modulo 1), and therefore
\[ (\bK_0)_{ij} = K((i - j)\Delta), \]
so $\bK_0$ is circulant.
In case $K$ is the Dirichlet kernel, we have $\bK_0 = \bF\bF^*$ where $\bF \in \CC^{s \times (2f_c + 1)}$ having, assuming $t_1 = \Delta$ as we may without loss of generality,
\[ F_{jf} = \exp(2\pi i t_j f) = \exp(2\pi i jf\Delta) = \exp\left(2\pi i \frac{jf}{s}\right). \]
That is, $\bF$ is nothing but a discrete Fourier transform (DFT) matrix with some redundant frequencies included.
In this formulation, we see immediately a simple significance of the threshold $\gamma = \frac{1}{2}$: it is the threshold below which $\bF$ is no longer a complete DFT matrix in dimension $s$.

In this case, the inverse of $\bK_0$ is related to $\bK_0$ itself, which may be seen by computing $\bF^*\bF$, where we see that the usual orthogonality of Fourier harmonics of order $s$ appears:
\[ (\bF^*\bF)_{fg} = \sum_{j = 0}^{s - 1}\exp\left(2\pi i \frac{j(g - f)}{s}\right) = s[f \equiv g \mod s]. \]

We have $(\bK_0)_{jj} = 2f_c + 1$, and
\begin{align*}
  (\bK_0)_{jk}
  &= \sum_{f = -f_c}^{f_c}\exp\left(2\pi i \frac{(k - j)f}{s}\right) \\
  &= \frac{\sin((f_c + \frac{1}{2})(k - j))}{\sin(\frac{1}{2}(k - j))}
\end{align*}

\section{Local Analysis}

\begin{proposition}
    Let $q: [0, 1] \to \CC^m$ be a smooth function.
    Write $N(t) = \|q(t)\|^2 = \sum_{k = 1}^m |q_k(t)|^2$.
    Then,
    \begin{align}
      N^\prime(t) &= 2\Re(\la q(t), q^\prime(t) \ra) \\
      N^{\prime\prime}(t) &= 2\Re(\la q(t), q^{\prime\prime}(t) \ra) + 2\|q^\prime(t)\|^2.
    \end{align}
\end{proposition}
Thus, for the local analysis of $N(t)$ near one of the $t_j$ in the special case of orthogonal signs, we have
\begin{align}
  N(t_k) &= 1 \\
  N^\prime(t_k) &= 0 \\
  N^{\prime\prime}(t_k) &= 2\Re(q^{\prime\prime}_k(t_k)) + 2\sum_{j \neq k}|q_j^\prime(t_k)|^2
\end{align}
Thus, the local concavity of $N(t)$ near $t_k$ is a battle between, on the one hand, the concavity of $q_k$ near $t_k$, and on the other hand the total magnitude of the derivatives in all other dimensions.
Since $K^{\prime\prime\prime}(0) = 0$, we may express
\[ q^{\prime\prime}_k(t_k) = \sum_{j = 1}^s \alpha_{jk}K^{\prime\prime}(t_k - t_j) = \la \balpha_k, \bkappa_{2, k} \ra, \]
where we substitute in the leading behavior derived in the previous section.

For the derivative terms, we have
\begin{align}
  q^\prime_j(t_k)
  &= \sum_{i = 1}^s\alpha_{ij}\left(K^\prime(t_k - t_i) - \frac{K^\prime(t_j - t_i)K^{\prime\prime}(t_k - t_j)}{K^{\prime\prime}(0)}\right) \\
  &= \la \balpha_j, \bkappa_{1, k} \ra - \frac{K^{\prime\prime}(t_k - t_j)}{K^{\prime\prime}(0)} \la \balpha_j, \bkappa_{1, j} \ra
\end{align}
The last inner product here may be written fairly simply,
\[ \la \balpha_j, \bkappa_{1, j} \ra = \bkappa_{1, j}^\top \bK_0^{-1} \be_j\left(1 - \frac{1}{\frac{K^{\prime\prime}(0)}{\bkappa_{1, k}^\top \bK_0^{-1}\bkappa_{1, k}} + 1}\right). \]

\paragraph{Heuristics}
If we assume the second term above is negligible, then the ``adversary term'' for local concavity would be of the form
\[ \sum_{j \neq k} |\la \balpha_j, \bkappa_{1, k} \ra|^2. \]
We may think heuristically of $\bkappa_{1, k}$ as having an equal positive and negative weight on entries $k \pm 1$ respectively, whereby this computes the total ``gradient'' of each $\balpha_j$ between entries $k \pm 1$.
Thus, to the extent that $\bK_0^{-1}$, the solution operator of Dirichlet interpolation, operates in a ``local'' way, the only significant contributions should come from the first few neighbors on either side of dimension $k$.
(Indeed, away from these neighbors we expect the second correction term to be small as well, since the leading factor will be small according to the decay of $K^{\prime\prime}$ away from zero.)

\subsection{Steps}
Assume orthogonal basis-generating samples (requires $ s\leq m $). For a given $ \epsilon>0 $ (of choice) and for all $ t_i $, $ 1\leq i \leq s $ satisfying minimum separation condition,
\paragraph{1. $ N^{\prime\prime}(t) $ stays negative for $ t \in [t_i-\epsilon, t_{i}+\epsilon] $} 
Together with the fact that $ N^\prime(t) = 2\Re(\la q(t), q^\prime(t) \ra) = 0  $ due to construction, this ensures $ N(t)\leq 1 $ for $ t \in [t_i-\epsilon, t_{i}+\epsilon] $. 
\begin{align}
	N^{\prime\prime}(t) &= 2\Re\{\la q(t), (q^{\prime\prime}(t))^* \ra\} + 2\|q^\prime(t)\|^2 \\
	& = 2\sum_{k = 1}^m \Re \{q_k(t) (q^{\prime\prime}_k(t))^*\} + |q_k^\prime(t)|^2 \\
	& = \sum_{k = 1}^m 2\Re\left\{\underbrace{\left(\sum_{j = 1}^s \alpha_{j, k} K(t - t_j) + \beta_kK^\prime(t - t_k)\right) \left(\sum_{j = 1}^s \alpha_{j, k}^* K^{\prime\prime}(t - t_j) + \beta_k^* K^{\prime\prime\prime}(t - t_k) \right)}_{x_k(t)}\right\} \\
	&+ \underbrace{\left(\sum_{j = 1}^s \alpha_{j, k} K^{\prime}(t - t_j) + \beta_kK^{\prime\prime}(t - t_k)\right) \left(\sum_{j = 1}^s \alpha_{j, k}^* K^{\prime}(t - t_j) + \beta_k^* K^{\prime\prime}(t - t_k) \right)}_{y_k(t)} 
\end{align}
\begin{align}
	x_k(t)& = \sum_{j = 1}^s\sum_{\ell = 1}^s \alpha_{j, k} K(t - t_j) \alpha_{\ell, k}^* K^{\prime\prime}(t - t_\ell) + \beta_k^* K^{\prime\prime\prime}(t - t_k) \sum_{j = 1}^s \alpha_{j, k} K(t - t_j) \\
	& +\beta_kK^\prime(t - t_k) \sum_{j = 1}^s \alpha_{j, k}^* K^{\prime\prime}(t - t_j) + \beta_kK^\prime(t - t_k) \beta_k^* K^{\prime\prime\prime}(t - t_k)  
\end{align}
For $ t = t_i+\epsilon $,
\begin{align}
	x_k(t) &=  \sum_{j = 1}^i\sum_{\ell = 1}^i \alpha_{j, k} \alpha_{\ell, k}^* K(\delta_{i,j}+\epsilon)  K^{\prime\prime}(\delta_{i,\ell} +\epsilon) + \sum_{j = i+1}^s\sum_{\ell = 1}^i \alpha_{j, k} \alpha_{\ell, k}^* K(\delta_{i,j}-\epsilon)  K^{\prime\prime}(\delta_{i,\ell} +\epsilon) \\
	& + \sum_{j = 1}^i\sum_{\ell = i+1}^s \alpha_{j, k} \alpha_{\ell, k}^* K(\delta_{i,j}+\epsilon)  K^{\prime\prime}(\delta_{i,\ell} -\epsilon)
	+ \sum_{j = i+1}^s\sum_{\ell = i+1}^s \alpha_{j, k} \alpha_{\ell, k}^* K(\delta_{i,j}-\epsilon)  K^{\prime\prime}(\delta_{i,\ell} -\epsilon) \\
	& + \beta_k^* K^{\prime\prime\prime}(t_i - t_k+\epsilon) \sum_{j = 1}^i \alpha_{j, k} K(\delta_{i,j} + \epsilon) + \beta_k^* K^{\prime\prime\prime}(t_i - t_k+\epsilon) \sum_{j = i+1}^s \alpha_{j, k} K(\delta_{i,j}-\epsilon)\\
	&+ \beta_k K^{\prime}(t_i - t_k+\epsilon) \sum_{j = 1}^i \alpha_{j, k}^* K^{\prime\prime}(\delta_{i,j} + \epsilon) + \beta_k K^{\prime}(t_i - t_k+\epsilon) \sum_{j = i+1}^s \alpha_{j, k}^* K^{\prime\prime}(\delta_{i,j}-\epsilon)\\
	&+  |\beta_k|^2 K^\prime(t_i - t_k+\epsilon)  K^{\prime\prime\prime}(t_i - t_k+\epsilon) 	
\end{align}
fejer kernel, distribution of  real alphas.
where $ \delta_{i,j} = |t_i-t_j| $. We assumed symmetric kernel.
\paragraph{2. $ N(t)<1 $ for $ t \in [t_{i}+\epsilon, t_{i+1}-\epsilon] $}


\paragraph{3. For non-orthogonal samples }
Duality question: Is solving the non-orthogonal case and assuming 

\clearpage

\section{Tools}

\subsection{Bernstein Polynomial Inequality}

The idea of using this comes from \cite{tang2013compressed}.
The inequality is as follows.

\begin{lemma}
    Let $p(z)$ be a complex polynomial of degree $N$.
    Then,
    \[ \sup_{|z| \leq 1} |p^\prime(z)| \leq N\sup_{|z| \leq 1}|p(z)|. \]
\end{lemma}
The way this is used in that paper is basically to argue that a well-bounded polynomial must also have a good Lipschitz property over the unit circle:
\begin{align*}
  |p(e^{ix}) - p(e^{iy})|
  &\leq |e^{-ix} - e^{iy}|\sup_{|z| = 1}|p^\prime(z)| \\
  &\leq \left(CN\sup_{|z| = 1}|p(z)|\right) |x - y|
\end{align*}
There, this is used to show that random polynomials are close to their mean, by showing this on a grid and then applying this Lipschitz property to pass from the grid to the entire interval.
It's not clear how this can help us, since there is no natural choice of ``comparison'' polynomial.

\subsection{Nazarov Inequality}

\begin{lemma}
    Let $p(t)$ be a complex trigonometric polynomial of degree $n$, and $E \subset [0, 1]$.
    Then,
    \[ \max_{t \in [0, 1]} |p(t)| \leq \left(\frac{C}{m(E)}\right)^{n - 1}\max_{t \in E}|p(t)|. \]
\end{lemma}


\bibliographystyle{unsrt}
\bibliography{main}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
